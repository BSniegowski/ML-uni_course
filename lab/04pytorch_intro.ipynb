{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import checker\n",
    "import utils"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# Preparing datasets\n",
    "torch.manual_seed(5)\n",
    "\n",
    "# Regression dataset - Boston housing (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "boston_data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "boston_target = raw_df.values[1::2, 2]\n",
    "\n",
    "boston_X = torch.tensor(boston_data, dtype=torch.float32)\n",
    "boston_y = torch.tensor(boston_target, dtype=torch.float32)\n",
    "boston_w = torch.randn(boston_X.shape[1], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "boston_data = (boston_X, boston_y, boston_w)\n",
    "\n",
    "# Multidimensional datasets\n",
    "dataset_5d = torch.randn([1000, 5], dtype=torch.float32)\n",
    "param_5d = torch.randn(5, requires_grad=True)\n",
    "\n",
    "dataset_20d = torch.randn([325, 20], dtype=torch.float32)\n",
    "param_20d = torch.randn(20, requires_grad=True)\n",
    "\n",
    "multi_datasets = [(dataset_5d, param_5d), (dataset_20d, param_20d)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def mean_squared_error(X: torch.Tensor, theta: torch.Tensor) -> torch.Tensor:\n",
    "    squared_distances = torch.sum(torch.square(X - theta), dim=-1)\n",
    "    return torch.mean(squared_distances)\n",
    "\n",
    "checker.check_4_1_mse(mean_squared_error, multi_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def mean_error(X: torch.Tensor, theta: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.mean(torch.linalg.norm(X - theta, dim = 1))\n",
    "\n",
    "checker.check_4_1_me(mean_error, multi_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def max_error(X: torch.Tensor, theta: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.max(torch.linalg.norm(X - theta, dim = 1))\n",
    "\n",
    "checker.check_4_1_max(max_error, multi_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def linear_regression_loss(X: torch.Tensor, w: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    squared_distances = (X @ w - y) ** 2\n",
    "    return torch.mean(squared_distances)\n",
    "\n",
    "checker.check_4_1_lin_reg(linear_regression_loss, boston_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def regularized_regression_loss(X: torch.Tensor, w: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    alpha = 0.2\n",
    "    squared_distances = (X @ w - y) ** 2\n",
    "    return torch.mean(squared_distances) + alpha * torch.dot(w, w)\n",
    "\n",
    "checker.check_4_1_reg_reg(regularized_regression_loss, boston_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "dataset_1d = utils.get_classification_dataset_1d()\n",
    "dataset_2d = utils.get_classification_dataset_2d()\n",
    "\n",
    "def calculate_accuracy(logistic_reg, X, y):\n",
    "    preds = logistic_reg.predict(X)\n",
    "    correct_n = (preds == y).float().sum().item()\n",
    "    return correct_n / len(y)\n",
    "\n",
    "def plot_dataset_1d(logistic_reg, dataset_1d):\n",
    "    plt.scatter(dataset_1d.data[:10], [0.5] * 10, c=\"purple\", label=\"0\")\n",
    "    plt.scatter(dataset_1d.data[10:], [0.5] * 10, c=\"yellow\", label=\"1\")\n",
    "    linspace = torch.linspace(-7.5, 15, steps=100).view(-1, 1)\n",
    "    plt.plot(\n",
    "        linspace.numpy().ravel(),\n",
    "        logistic_reg.predict_proba(linspace).detach().numpy(),\n",
    "        label=\"p(y=1 | x)\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_dataset_2d(logistic_reg, dataset_2d):\n",
    "    plt.scatter(dataset_2d.data[:50, 0], dataset_2d.data[:50, 1], c=\"purple\", label=\"0\")\n",
    "    plt.scatter(dataset_2d.data[50:, 0], dataset_2d.data[50:, 1], c=\"yellow\", label=\"1\")\n",
    "\n",
    "    linspace_x = torch.linspace(-4, 7, steps=100)\n",
    "    linspace_y = (-logistic_reg.bias - logistic_reg.weight[0] * linspace_x) / logistic_reg.weight[1]\n",
    "\n",
    "    linspace_y = linspace_y.detach().numpy()\n",
    "    plt.plot(linspace_x.detach().numpy(), linspace_y, label=\"Granica decyzyjna\")\n",
    "    plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, input_dim):\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1/(1 + torch.exp(-x))\n",
    "\n",
    "    def fit(self, X, y, lr=1e-6, num_steps=int(1e4)):\n",
    "        self.weight = torch.randn(self.input_dim, requires_grad=True)\n",
    "        self.bias = torch.randn((), requires_grad=True)\n",
    "        for idx in range(num_steps):\n",
    "            self.weight.requires_grad = True\n",
    "            self.bias.requires_grad = True\n",
    "\n",
    "            loss_val = self.loss(X, y)\n",
    "            loss_val.backward()\n",
    "\n",
    "            w_grad = self.weight.grad\n",
    "            b_grad = self.bias.grad\n",
    "            with torch.no_grad():\n",
    "                self.weight = self.weight - lr * w_grad\n",
    "                self.bias = self.bias - lr * b_grad\n",
    "\n",
    "\n",
    "    def predict_proba(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return self._sigmoid(X @ self.weight + self.bias)\n",
    "\n",
    "    def greater_than_half(self, x):\n",
    "        if x < 0.5:\n",
    "            return float(0)\n",
    "        return float(1)\n",
    "\n",
    "    def predict(self, X: torch.Tensor) -> torch.FloatTensor:\n",
    "        z = self.predict_proba(X)\n",
    "        return z.apply_(self.greater_than_half)\n",
    "\n",
    "    def loss(self, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        z = X @ self.weight + self.bias\n",
    "        y2 = self._sigmoid(z)\n",
    "        return torch.mean((y-1) * torch.log(1-y2) - (y * torch.log(y2)))\n",
    "\n",
    "\n",
    "checker.check_04_logistic_reg(LogisticRegression)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "input = torch.randn(30, 20, dtype=torch.double, requires_grad=True) * 3\n",
    "a = torch.randn(20, 30, requires_grad=True).double() * 2 - 5\n",
    "b = torch.randn(20, 30, requires_grad=True).double() + 6\n",
    "\n",
    "\n",
    "preds = torch.rand(30, requires_grad=True).double()\n",
    "labels_dist = torch.distributions.Bernoulli(probs=0.7)\n",
    "labels = labels_dist.sample([30]).double()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyAdd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, a, b):\n",
    "        self.save_for_backward(a, b)\n",
    "        return a + b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        a, b = self.saved_tensors\n",
    "        a_grad = 1\n",
    "        b_grad = 1\n",
    "        return grad_output * a_grad, grad_output * b_grad\n",
    "\n",
    "add_fn = MyAdd.apply\n",
    "torch.autograd.gradcheck(add_fn, (a, b), eps=1e-3, atol=1e-2, rtol=1e-2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDiv(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, a, b):\n",
    "        self.save_for_backward(a, b)\n",
    "        return a / b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        a, b = self.saved_tensors\n",
    "        a_grad = 1/b\n",
    "        b_grad = -a/(b**2)\n",
    "        return grad_output * a_grad, grad_output * b_grad\n",
    "\n",
    "div_fn = MyDiv.apply\n",
    "torch.autograd.gradcheck(div_fn, (a, b), eps=1e-3, atol=1e-2, rtol=1e-2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySigmoid(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, input_):\n",
    "        self.save_for_backward(input_)\n",
    "        return 1/(1 + torch.exp(-input_))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        input_, = self.saved_tensors\n",
    "        return grad_output * 1/(1 + torch.exp(-input_)) * (1 - 1/(1 + torch.exp(-input_)))\n",
    "\n",
    "\n",
    "sigmoid_fn = MySigmoid.apply\n",
    "torch.autograd.gradcheck(sigmoid_fn, input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "function MyBinaryCrossEntropyBackward returned an incorrect number of gradients (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/yc/4qc9kh053_q6byxpyyr_7xdm0000gn/T/ipykernel_62689/2550398426.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mbce_fn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMyBinaryCrossEntropy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradcheck\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbce_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mpreds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0matol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrtol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.7/site-packages/torch/autograd/gradcheck.py\u001B[0m in \u001B[0;36mgradcheck\u001B[0;34m(func, inputs, eps, atol, rtol, raise_exception, check_sparse_nnz, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode)\u001B[0m\n\u001B[1;32m   1412\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1413\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1414\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_gradcheck_helper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1415\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1416\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.7/site-packages/torch/autograd/gradcheck.py\u001B[0m in \u001B[0;36m_gradcheck_helper\u001B[0;34m(func, inputs, eps, atol, rtol, check_sparse_nnz, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode)\u001B[0m\n\u001B[1;32m   1429\u001B[0m                          \u001B[0mrtol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0matol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheck_grad_dtypes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheck_forward_ad\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcheck_forward_ad\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1430\u001B[0m                          \u001B[0mcheck_backward_ad\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcheck_backward_ad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnondet_tol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnondet_tol\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1431\u001B[0;31m                          check_undefined_grad=check_undefined_grad)\n\u001B[0m\u001B[1;32m   1432\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1433\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcheck_batched_forward_grad\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.7/site-packages/torch/autograd/gradcheck.py\u001B[0m in \u001B[0;36m_gradcheck_real_imag\u001B[0;34m(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, check_forward_ad, check_backward_ad, nondet_tol, check_undefined_grad)\u001B[0m\n\u001B[1;32m   1074\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1075\u001B[0m             gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,\n\u001B[0;32m-> 1076\u001B[0;31m                          rtol, atol, check_grad_dtypes, nondet_tol)\n\u001B[0m\u001B[1;32m   1077\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1078\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcheck_forward_ad\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.7/site-packages/torch/autograd/gradcheck.py\u001B[0m in \u001B[0;36m_slow_gradcheck\u001B[0;34m(func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, nondet_tol, use_forward_ad, complex_indices, test_imag)\u001B[0m\n\u001B[1;32m   1122\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1123\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mo\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1124\u001B[0;31m             \u001B[0manalytical\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_check_analytical_jacobian_attributes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtupled_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mo\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnondet_tol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheck_grad_dtypes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1125\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1126\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manalytical\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumerical\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.7/site-packages/torch/autograd/gradcheck.py\u001B[0m in \u001B[0;36m_check_analytical_jacobian_attributes\u001B[0;34m(inputs, output, nondet_tol, check_grad_dtypes, fast_mode, v)\u001B[0m\n\u001B[1;32m    534\u001B[0m         \u001B[0mvjps2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_analytical_vjps_wrt_specific_output\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvjp_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclone\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    535\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 536\u001B[0;31m         \u001B[0mvjps1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_compute_analytical_jacobian_rows\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvjp_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclone\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    537\u001B[0m         \u001B[0mvjps2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_compute_analytical_jacobian_rows\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvjp_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclone\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    538\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.7/site-packages/torch/autograd/gradcheck.py\u001B[0m in \u001B[0;36m_compute_analytical_jacobian_rows\u001B[0;34m(vjp_fn, sample_output)\u001B[0m\n\u001B[1;32m    626\u001B[0m         \u001B[0mflat_grad_out\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    627\u001B[0m         \u001B[0mflat_grad_out\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1.0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 628\u001B[0;31m         \u001B[0mgrad_inputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvjp_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgrad_out_base\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    629\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0md_x\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgrad_inputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    630\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mj\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.7/site-packages/torch/autograd/gradcheck.py\u001B[0m in \u001B[0;36mvjp_fn\u001B[0;34m(grad_output)\u001B[0m\n\u001B[1;32m    528\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mvjp_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgrad_output\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    529\u001B[0m         return torch.autograd.grad(output, diff_input_list, grad_output,\n\u001B[0;32m--> 530\u001B[0;31m                                    retain_graph=True, allow_unused=True)\n\u001B[0m\u001B[1;32m    531\u001B[0m     \u001B[0;31m# Compute everything twice to check for nondeterminism (which we call reentrancy)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    532\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mfast_mode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.7/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mgrad\u001B[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001B[0m\n\u001B[1;32m    276\u001B[0m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[1;32m    277\u001B[0m             \u001B[0mt_outputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_outputs_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mt_inputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 278\u001B[0;31m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[1;32m    279\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    280\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: function MyBinaryCrossEntropyBackward returned an incorrect number of gradients (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "class MyBinaryCrossEntropy(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, preds, labels, bias=None):\n",
    "        self.save_for_backward(preds, labels)\n",
    "        return torch.mean((labels-1) * torch.log(1-preds) - labels * torch.log(preds))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        preds, labels = self.saved_tensors\n",
    "        grad_labels = None\n",
    "        return grad_output * ((preds - labels)/(preds * (1 - labels))) * (1/labels.size(dim=0))\n",
    "\n",
    "bce_fn = MyBinaryCrossEntropy.apply\n",
    "torch.autograd.gradcheck(bce_fn, (preds, labels), eps=1e-3, atol=1e-2, rtol=1e-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
