{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import checker\n",
    "import utils"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Preparing datasets\n",
    "torch.manual_seed(5)\n",
    "\n",
    "# Regression dataset - Boston housing (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "boston_data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "boston_target = raw_df.values[1::2, 2]\n",
    "\n",
    "boston_X = torch.tensor(boston_data, dtype=torch.float32)\n",
    "boston_y = torch.tensor(boston_target, dtype=torch.float32)\n",
    "boston_w = torch.randn(boston_X.shape[1], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "boston_data = (boston_X, boston_y, boston_w)\n",
    "\n",
    "# Multidimensional datasets\n",
    "dataset_5d = torch.randn([1000, 5], dtype=torch.float32)\n",
    "param_5d = torch.randn(5, requires_grad=True)\n",
    "\n",
    "dataset_20d = torch.randn([325, 20], dtype=torch.float32)\n",
    "param_20d = torch.randn(20, requires_grad=True)\n",
    "\n",
    "multi_datasets = [(dataset_5d, param_5d), (dataset_20d, param_20d)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mean_squared_error(X: torch.Tensor, theta: torch.Tensor) -> torch.Tensor:\n",
    "    squared_distances = torch.sum(torch.square(X - theta), dim=-1)\n",
    "    return torch.mean(squared_distances)\n",
    "\n",
    "checker.check_4_1_mse(mean_squared_error, multi_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mean_error(X: torch.Tensor, theta: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.mean(torch.linalg.norm(X - theta, dim = 1))\n",
    "\n",
    "checker.check_4_1_me(mean_error, multi_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def max_error(X: torch.Tensor, theta: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.max(torch.linalg.norm(X - theta, dim = 1))\n",
    "\n",
    "checker.check_4_1_max(max_error, multi_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def linear_regression_loss(X: torch.Tensor, w: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    squared_distances = (X @ w - y) ** 2\n",
    "    return torch.mean(squared_distances)\n",
    "\n",
    "checker.check_4_1_lin_reg(linear_regression_loss, boston_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def regularized_regression_loss(X: torch.Tensor, w: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    alpha = 0.2\n",
    "    squared_distances = (X @ w - y) ** 2\n",
    "    return torch.mean(squared_distances) + alpha * torch.dot(w, w)\n",
    "\n",
    "checker.check_4_1_reg_reg(regularized_regression_loss, boston_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_1d = utils.get_classification_dataset_1d()\n",
    "dataset_2d = utils.get_classification_dataset_2d()\n",
    "\n",
    "def calculate_accuracy(logistic_reg, X, y):\n",
    "    preds = logistic_reg.predict(X)\n",
    "    correct_n = (preds == y).float().sum().item()\n",
    "    return correct_n / len(y)\n",
    "\n",
    "def plot_dataset_1d(logistic_reg, dataset_1d):\n",
    "    plt.scatter(dataset_1d.data[:10], [0.5] * 10, c=\"purple\", label=\"0\")\n",
    "    plt.scatter(dataset_1d.data[10:], [0.5] * 10, c=\"yellow\", label=\"1\")\n",
    "    linspace = torch.linspace(-7.5, 15, steps=100).view(-1, 1)\n",
    "    plt.plot(\n",
    "        linspace.numpy().ravel(),\n",
    "        logistic_reg.predict_proba(linspace).detach().numpy(),\n",
    "        label=\"p(y=1 | x)\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_dataset_2d(logistic_reg, dataset_2d):\n",
    "    plt.scatter(dataset_2d.data[:50, 0], dataset_2d.data[:50, 1], c=\"purple\", label=\"0\")\n",
    "    plt.scatter(dataset_2d.data[50:, 0], dataset_2d.data[50:, 1], c=\"yellow\", label=\"1\")\n",
    "\n",
    "    linspace_x = torch.linspace(-4, 7, steps=100)\n",
    "    linspace_y = (-logistic_reg.bias - logistic_reg.weight[0] * linspace_x) / logistic_reg.weight[1]\n",
    "\n",
    "    linspace_y = linspace_y.detach().numpy()\n",
    "    plt.plot(linspace_x.detach().numpy(), linspace_y, label=\"Granica decyzyjna\")\n",
    "    plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, input_dim):\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1/(1 + torch.exp(-x))\n",
    "\n",
    "    def fit(self, X, y, lr=1e-6, num_steps=int(1e4)):\n",
    "        self.weight = torch.randn(self.input_dim, requires_grad=True)\n",
    "        self.bias = torch.randn((), requires_grad=True)\n",
    "        for idx in range(num_steps):\n",
    "            self.weight.requires_grad = True\n",
    "            self.bias.requires_grad = True\n",
    "\n",
    "            loss_val = self.loss(X, y)\n",
    "            loss_val.backward()\n",
    "\n",
    "            w_grad = self.weight.grad\n",
    "            b_grad = self.bias.grad\n",
    "            with torch.no_grad():\n",
    "                self.weight = self.weight - lr * w_grad\n",
    "                self.bias = self.bias - lr * b_grad\n",
    "\n",
    "\n",
    "    def predict_proba(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return self._sigmoid(X @ self.weight + self.bias)\n",
    "\n",
    "    def greater_than_half(self, x):\n",
    "        if x < 0.5:\n",
    "            return float(0)\n",
    "        return float(1)\n",
    "\n",
    "    def predict(self, X: torch.Tensor) -> torch.FloatTensor:\n",
    "        z = self.predict_proba(X)\n",
    "        return z.apply_(self.greater_than_half)\n",
    "\n",
    "    def loss(self, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        z = X @ self.weight + self.bias\n",
    "        y2 = self._sigmoid(z)\n",
    "        return torch.mean((y-1) * torch.log(1-y2) - (y * torch.log(y2)))\n",
    "\n",
    "\n",
    "checker.check_04_logistic_reg(LogisticRegression)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input = torch.randn(30, 20, dtype=torch.double, requires_grad=True) * 3\n",
    "a = torch.randn(20, 30, requires_grad=True).double() * 2 - 5\n",
    "b = torch.randn(20, 30, requires_grad=True).double() + 6\n",
    "\n",
    "\n",
    "preds = torch.rand(30, requires_grad=True).double()\n",
    "labels_dist = torch.distributions.Bernoulli(probs=0.7)\n",
    "labels = labels_dist.sample([30]).double()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyAdd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, a, b):\n",
    "        self.save_for_backward(a, b)\n",
    "        return a + b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        a, b = self.saved_tensors\n",
    "        a_grad = 1\n",
    "        b_grad = 1\n",
    "        return grad_output * a_grad, grad_output * b_grad\n",
    "\n",
    "add_fn = MyAdd.apply\n",
    "torch.autograd.gradcheck(add_fn, (a, b), eps=1e-3, atol=1e-2, rtol=1e-2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyDiv(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, a, b):\n",
    "        self.save_for_backward(a, b)\n",
    "        return a / b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        a, b = self.saved_tensors\n",
    "        a_grad = 1/b\n",
    "        b_grad = -a/(b**2)\n",
    "        return grad_output * a_grad, grad_output * b_grad\n",
    "\n",
    "div_fn = MyDiv.apply\n",
    "torch.autograd.gradcheck(div_fn, (a, b), eps=1e-3, atol=1e-2, rtol=1e-2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MySigmoid(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, input_):\n",
    "        self.save_for_backward(input_)\n",
    "        return 1/(1 + torch.exp(-input_))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        input_, = self.saved_tensors\n",
    "        return grad_output * 1/(1 + torch.exp(-input_)) * (1 - 1/(1 + torch.exp(-input_)))\n",
    "\n",
    "\n",
    "sigmoid_fn = MySigmoid.apply\n",
    "torch.autograd.gradcheck(sigmoid_fn, input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyBinaryCrossEntropy(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, preds, labels, bias=None):\n",
    "        self.save_for_backward(preds, labels)\n",
    "        return torch.mean((labels-1) * torch.log(1-preds) - labels * torch.log(preds))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        preds, labels = self.saved_tensors\n",
    "        grad_labels = None\n",
    "        # Why this works??\n",
    "        return grad_output * ((-labels/preds + (1-labels)/(1-preds)) / labels.size(dim=0)), grad_labels\n",
    "\n",
    "bce_fn = MyBinaryCrossEntropy.apply\n",
    "torch.autograd.gradcheck(bce_fn, (preds, labels), eps=1e-3, atol=1e-2, rtol=1e-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
