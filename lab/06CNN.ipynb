{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ploting\n",
    "%matplotlib inline\n",
    "# imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data loading\n",
    "cifar_sample = np.load('resources/cifar_sample.npy')\n",
    "# get a first random image\n",
    "np_image = cifar_sample[0]\n",
    "# this should plot a blurry frog\n",
    "plt.imshow(np_image.transpose(1,2,0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import math\n",
    "\n",
    "def convolution(image: torch.tensor,\n",
    "                filters: torch.tensor,\n",
    "                bias: torch.tensor,\n",
    "                stride: int = 1,\n",
    "                padding: int = 1):\n",
    "    \"\"\"\n",
    "    :param image: torch.Tensor\n",
    "        Input image of shape (C, H, W)\n",
    "    :param filters: torch.Tensor\n",
    "        Filters to use in convolution of shape (K, C, F, F)\n",
    "    :param bias: torch.Tensor\n",
    "        Bias vector of shape (K,)\n",
    "    :param stride: int\n",
    "        Stride to use in convolution\n",
    "    :param padding: int\n",
    "       Zero-padding to add on all sides of the image\n",
    "    \"\"\"\n",
    "    # get image dimensions\n",
    "    img_channels, img_height, img_width = image.shape\n",
    "    n_filters, filter_channels, filter_size, filter_size = filters.shape\n",
    "    # should img_channels == filter_channels ??\n",
    "    assert img_channels == filter_channels , 'Number of image channels should equal number of filter channels'\n",
    "\n",
    "    # calculate the dimensions of the output image\n",
    "    out_height = math.floor((2*padding + img_height - filter_size)/stride) + 1\n",
    "    out_width = math.floor((2*padding + img_width - filter_size)/stride) + 1\n",
    "    out_channels = n_filters\n",
    "\n",
    "    img_for_iterating = transforms.Pad(padding=padding, fill=0).forward(img=image)\n",
    "    output = torch.FloatTensor(size=(out_channels, out_height, out_width))\n",
    "\n",
    "    for c in range(out_channels):\n",
    "        for h in range(out_height):\n",
    "            for w in range(out_width):\n",
    "                val = 0\n",
    "                for i in range(filter_channels):\n",
    "                    tmp_h = stride * h\n",
    "                    tmp_w = stride * w\n",
    "                    img_fragment = img_for_iterating[i, tmp_h:tmp_h+filter_size, tmp_w:tmp_w+filter_size]\n",
    "                    val += torch.tensordot(img_fragment.float(), filters[c, i])\n",
    "                output[c, h, w] = val + bias[c]\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "# Convolution Test\n",
    "\n",
    "# cast the frog to tensor\n",
    "image = torch.tensor(np_image)\n",
    "# preapre parameters for testing\n",
    "paddings = [0, 1, 2, 3]\n",
    "strides = [1, 2, 3, 4]\n",
    "filters = [(torch.randn((2,3,3,3)), torch.randn((2))),\n",
    "           (torch.randn((2,3,5,5)), torch.randn((2))),\n",
    "           (torch.randn((5,3,1,1)), torch.randn((5)))]\n",
    "\n",
    "# test all combinations\n",
    "for (filt, bias), stride, padding in product(filters, strides, paddings):\n",
    "    # your convolution\n",
    "    out = convolution(image, filt, bias, stride=stride, padding=padding)\n",
    "    # PyTorch equivalent\n",
    "    out_torch = torch.conv2d(input=image.unsqueeze(0), weight=filt, bias=bias, padding=padding, stride=stride)\n",
    "    # asserts\n",
    "    assert out_torch.squeeze().shape == out.shape\n",
    "    assert torch.allclose(out, out_torch.squeeze(), atol=1e-5, rtol=1e-5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "def max_pooling(image: torch.tensor,\n",
    "                kernel_size: int,\n",
    "                stride: int = 1,\n",
    "                padding: int = 1):\n",
    "    \"\"\"\n",
    "    :param image: torch.Tensor\n",
    "        Input image of shape (C, H, W)\n",
    "    :param kernel_size: int\n",
    "        Size of the square pooling kernel\n",
    "    :param stride: int\n",
    "        Stride to use in pooling\n",
    "    :param padding: int\n",
    "       Zero-padding to add on all sides of the image\n",
    "    \"\"\"\n",
    "    # get image dimensions\n",
    "    img_channels, img_height, img_width = image.shape\n",
    "\n",
    "    # calculate the dimensions of the output image\n",
    "    out_height = math.floor((2*padding + img_height - kernel_size)/stride) + 1\n",
    "    out_width = math.floor((2*padding + img_width - kernel_size)/stride) + 1\n",
    "    out_channels = img_channels\n",
    "\n",
    "    img_for_iterating = transforms.Pad(padding=padding, fill=0).forward(img=image)\n",
    "    output = torch.FloatTensor(size=(out_channels, out_height, out_width))\n",
    "\n",
    "    for c in range(out_channels):\n",
    "        for h in range(out_height):\n",
    "            for w in range(out_width):\n",
    "                tmp_h = stride * h\n",
    "                tmp_w = stride * w\n",
    "                img_fragment = img_for_iterating[c, tmp_h:tmp_h+kernel_size, tmp_w:tmp_w+kernel_size]\n",
    "                output[c, h, w] = torch.max(img_fragment)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "# Max Pooling Test\n",
    "from itertools import product\n",
    "\n",
    "# cast the frog to tensor\n",
    "image = torch.tensor(np_image)\n",
    "# preapre parameters for testing\n",
    "kernel_sizes = [2, 3, 4]\n",
    "paddings = [0, 1]\n",
    "strides = [1, 2, 3, 4]\n",
    "\n",
    "# test all combinations\n",
    "for kernel_size, stride, padding in product(kernel_sizes, strides, paddings):\n",
    "    # your pooling\n",
    "    out = max_pooling(image, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    # PyTorch equivalent\n",
    "    out_torch = torch.nn.functional.max_pool2d(input=image.unsqueeze(0), kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "    # asserts\n",
    "    assert out_torch.squeeze().shape == out.shape\n",
    "    assert torch.allclose(out, out_torch.squeeze(), atol=1e-5, rtol=1e-5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
