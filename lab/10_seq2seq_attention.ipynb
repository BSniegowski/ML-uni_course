{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BSniegowski/ML-uni_course/blob/main/lab/10_seq2seq_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n1plaPa_y4K"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "t3ph3l6HACXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/gmum/ml2022-23/master/lab/resources/eng-pol.txt"
      ],
      "metadata": {
        "id": "2mdk_jJzAGO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
        "        self.n_words = 3 # Count SOS, EOS and PAD\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "Wpd0BSiNANd1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = s.replace(\"ł\", \"l\")\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('eng-pol.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    lines = lines[1:]  # Skip first line with attributions.\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[1::2]] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "Uo3FBGilARL4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 20\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) <= MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) <= MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "CQFv6NBaAT6f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(pairs[0])\n",
        "    print(f\"Read {len(pairs)} sentence pairs\")\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('pol', 'eng', True)\n",
        "print(\"Przykładowe pary zdań:\")\n",
        "for _ in range(3):\n",
        "    print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSTieO9eAYQ1",
        "outputId": "f51103f1-bfbb-4456-bc50-7681b96abc9d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "['sprobujmy cos .', 'let s try something .']\n",
            "Read 59749 sentence pairs\n",
            "Trimmed to 59404 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 29246\n",
            "pol 11953\n",
            "Przykładowe pary zdań:\n",
            "['swiatlo przenika mrok .', 'the light penetrates the darkness .']\n",
            "['herbata bez lodu .', 'tea without ice .']\n",
            "['nie rozumiem czemu mialem przyjsc .', 'i don t see why i had to come along .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "\n",
        "def pad_sequences(data_batch):\n",
        "    pl_batch, en_batch = [], []\n",
        "    for pl_sentence, en_sentence in data_batch:\n",
        "        pl_batch += [pl_sentence]\n",
        "        en_batch += [en_sentence]\n",
        "    pl_batch = pad_sequence(pl_batch, padding_value=PAD_token, batch_first=True)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_token, batch_first=True)\n",
        "    return pl_batch, en_batch\n",
        "\n",
        "def prepare_dataset(batch_size):\n",
        "    rng = np.random.RandomState(567)\n",
        "    indices = np.arange(len(pairs))\n",
        "    rng.shuffle(indices)\n",
        "    train_indices = indices[:int(len(pairs) * 0.8)]\n",
        "    test_indices = indices[int(len(pairs) * 0.8):]\n",
        "    train_pairs = list(pairs[idx] for idx in train_indices)\n",
        "    test_pairs = list(pairs[idx] for idx in test_indices)\n",
        "    tensor_train_pairs = [tensorsFromPair(pairs[idx]) for idx in train_indices]\n",
        "    tensor_test_pairs = [tensorsFromPair(pairs[idx]) for idx in test_indices]\n",
        "    reference_translation = test_pairs\n",
        "\n",
        "    # Output in natural language?\n",
        "\n",
        "    train_loader = DataLoader(tensor_train_pairs, batch_size=batch_size,\n",
        "                            shuffle=True, collate_fn=pad_sequences)\n",
        "    test_loader = DataLoader(tensor_test_pairs, batch_size=batch_size,\n",
        "                            shuffle=True, collate_fn=pad_sequences)\n",
        "    return train_pairs, test_pairs, train_loader, test_loader"
      ],
      "metadata": {
        "id": "mKQGsdNrAd_f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "\n",
        "def predict(encoder, decoder, inputs, targets=None, max_len=MAX_LENGTH):\n",
        "    batch_size = inputs.size(0)\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(inputs)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]] * batch_size, device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_output, decoder_attention = decoder(\n",
        "        decoder_input,\n",
        "        decoder_hidden,\n",
        "        targets=targets,\n",
        "        max_len=max_len,\n",
        "        encoder_outputs=encoder_outputs)\n",
        "    return decoder_output, decoder_attention\n",
        "\n",
        "def translate(encoder, decoder, sentence, show_attention=True):\n",
        "    inputs = tensorFromSentence(input_lang, sentence).unsqueeze(0).cuda()\n",
        "    decoder_output, decoder_attention = predict(encoder, decoder, inputs)\n",
        "\n",
        "    decoded_words = []\n",
        "    for word in decoder_output[0]:\n",
        "        top_word = word.argmax(-1).item()\n",
        "        decoded_words.append(output_lang.index2word[top_word])\n",
        "        if top_word == EOS_token:\n",
        "            break\n",
        "\n",
        "    if decoder_attention is not None and show_attention:\n",
        "        # [out_words, in_words]\n",
        "        att = decoder_attention.cpu().detach().numpy()\n",
        "        att = att[0, :len(decoded_words), :]\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        im = ax.imshow(att, vmin=0, vmax=1)\n",
        "        ax.xaxis.tick_top()\n",
        "        ax.set_xticklabels([''] + sentence.split(' ') +\n",
        "                        ['EOS'], rotation=90)\n",
        "        ax.set_yticklabels([''] + decoded_words)\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "\n",
        "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "        \n",
        "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    return decoded_words\n",
        "\n",
        "def batch_translate(encoder, decoder, batch):\n",
        "    decoder_output, decoder_attention = predict(encoder, decoder, batch)\n",
        "\n",
        "    predicted_sentences = []\n",
        "\n",
        "    # TODO: potentially paralellize?\n",
        "    for batch_idx in range(len(batch)):\n",
        "        predicted_words = []\n",
        "        for word in decoder_output[batch_idx]:\n",
        "            top_word = word.argmax(-1).item()\n",
        "            if top_word == EOS_token:\n",
        "                break\n",
        "            predicted_words.append(output_lang.index2word[top_word])\n",
        "\n",
        "        predicted_sentences.append(predicted_words)\n",
        "\n",
        "    return predicted_sentences\n",
        "\n",
        "def dataset_translate(encoder, decoder, loader):\n",
        "    predicted_sentences = []\n",
        "    reference_sentences = [] \n",
        "    for batch_in, batch_out in loader:\n",
        "        translated = batch_translate(encoder, decoder, batch_in)\n",
        "        predicted_sentences.extend(translated)\n",
        "\n",
        "        # TODO: move to a separate file?\n",
        "        reference_words = []\n",
        "        for sentence_idx, sentence in enumerate(batch_out):\n",
        "            decoded_sentence = []\n",
        "            for word in sentence:\n",
        "                if word.item() == EOS_token:\n",
        "                    break\n",
        "                decoded_sentence.append(output_lang.index2word[word.item()])\n",
        "            reference_sentences.append(decoded_sentence)\n",
        "    \n",
        "    return predicted_sentences, reference_sentences\n",
        "\n",
        "\n",
        "def translate_randomly(encoder, decoder, pairs, n=10):\n",
        "    # TODO: reuse translate_given_pairs\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = translate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "def translate_given_pairs(encoder, decoder, pairs):\n",
        "    for pair in pairs:\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = translate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "        \n",
        "def plot_results(bleus, losses):\n",
        "    sns.set_style('whitegrid')\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
        "\n",
        "    axes[0].plot(np.arange(len(bleus)), bleus)\n",
        "    axes[0].set_xlabel(\"Epoka\")\n",
        "    axes[0].set_ylabel(\"BLEU\")\n",
        "    axes[1].plot(np.arange(len(losses)), losses)\n",
        "    axes[1].set_xlabel(\"Epoka\")\n",
        "    axes[1].set_ylabel(\"Koszt na zbiorze treningowym\")"
      ],
      "metadata": {
        "id": "8oKYfkncE2T8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(encoder, decoder, lr=0.01, batch_size=256, teacher_forcing_ratio=0.5, epochs_num=100, clipping=1.0):\n",
        "\n",
        "    # Prepare dataset, loss functions, optimizer\n",
        "    train_pairs, test_pairs, train_loader, test_loader = prepare_dataset(batch_size)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()\n",
        "\n",
        "    bleus = []\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(epochs_num + 1):\n",
        "\n",
        "        # Training\n",
        "        epoch_train_loss = 0.\n",
        "        for in_batch, out_batch in train_loader:\n",
        "            in_batch, out_batch = in_batch.cuda(), out_batch.cuda()\n",
        "\n",
        "            encoder_optimizer.zero_grad()\n",
        "            decoder_optimizer.zero_grad()\n",
        "        \n",
        "            teacher_inputs = out_batch if random.random() < teacher_forcing_ratio else None\n",
        "        \n",
        "            decoder_output, decoded_attention = predict(\n",
        "                encoder, decoder, in_batch,\n",
        "                targets=teacher_inputs,\n",
        "                max_len=out_batch.size(1)\n",
        "            )\n",
        "            \n",
        "            loss = criterion(decoder_output.transpose(1, 2), out_batch)\n",
        "            \n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(encoder.parameters(), clipping)\n",
        "            nn.utils.clip_grad_norm_(decoder.parameters(), clipping)\n",
        "\n",
        "            encoder_optimizer.step()\n",
        "            decoder_optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        # Evaluation\n",
        "        if epoch % 1 == 0:\n",
        "            with torch.no_grad():\n",
        "                print(\"=\" * 25, \"Translation test\", \"=\" * 25)\n",
        "                translate_randomly(encoder, decoder, test_pairs, n=5)\n",
        "\n",
        "            pred_sentences, ref_sentences = dataset_translate(encoder, decoder, test_loader)\n",
        "            bleu_val = bleu_score(pred_sentences, [[sentence] for sentence in ref_sentences])\n",
        "            print(\"=\" * 25, f\"BLEU: {bleu_val}\", \"=\" * 25)\n",
        "            bleus += [bleu_val]\n",
        "\n",
        "        mean_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses += [mean_train_loss]\n",
        "        print(f\"Epoch: {epoch}. Train loss: {mean_train_loss}\")\n",
        "    return bleus, train_losses"
      ],
      "metadata": {
        "id": "DdyKvKqOE8-_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_cell = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.rnn_cell(embedded)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "aS2q1pzVFBIL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
        "        self.reshape = nn.Linear(hidden_size, embedding_size)\n",
        "\n",
        "    def forward(self, input, hidden, targets=None, max_len=None, encoder_outputs=None):\n",
        "      input = self.embedding(input)\n",
        "      output = input\n",
        "      for i in range(max_len):\n",
        "        # print(input.size(), hidden.size())\n",
        "        input, hidden = self.gru(input, hidden)\n",
        "        input = self.reshape(input)\n",
        "        output = torch.cat((output, input), 1)\n",
        "      return output, None\n",
        "\n",
        "hidden_size = 1024\n",
        "embedding_size = 512\n",
        "lr = 1e-3\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "decoder = DecoderRNN(output_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "\n",
        "bleus, losses = train(encoder, decoder, batch_size=512, lr=lr, epochs_num=10, clipping=0.1)\n",
        "plot_results(bleus, losses)"
      ],
      "metadata": {
        "id": "dXBpZ4rKFB8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}